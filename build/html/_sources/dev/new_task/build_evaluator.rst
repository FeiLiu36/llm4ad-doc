Define your `Sampler`
=========================

.. note::
    The `Sampler` class (an abstract class) defines how to access the LLM.
    You can either deploy an LLM locally on your own device/server or use an LLM API.
    The user should create a new child class of the `Sampler` class (extend `Sampler`) and implement (override) the `draw_sample` function.

Initialization of the user-defined sampler class
-----------------------------------------------------

There is a keyword argument `auto_trim` in the `Sampler` class, with a default value of `True`. This means that regardless of whether the user chooses a code completion model (such as StarCoder, CodeLlama-Python, etc.) or a chat model (GPT series, Llama series, etc.), we can automatically identify the “useful part” without descriptions and truncated code.

.. tip::
    Therefore, unless there is a special issue, please **always leave 'auto_trim' default**.

Implementation of the draw_sample function
-----------------------------------------------------

The `draw_sample` function decides how to obtain the generated content from the LLM and return the `str`-typed content

.. note::
    feel free to return the answer generated by LLM, which may incorporate some useless descriptions, as they will be trimmed automatically by our trimmer).

Here, we show a brief example of using LLM API.

.. code:: python

    from llm4ad.tools.llm.llm_api_https import HttpsApi

    # note that the 'host' has no 'https'
    sampler = HttpsApi(host='api.bltcy.ai', key='Your API key', model='gpt-3.5-turbo', timeout=30)

You can also implement your own sampler.

.. code:: python

    import llm4ad
    import time
    import http.client
    import json

    class MySampler(llm4ad.base.Sampler):
        def __init__(self):
            super().__init__()

        def draw_sample(self, prompt: str | Any, *args, **kwargs) -> str:
            while True:
                try:
                    conn = http.client.HTTPSConnection(f'{api_endpoint}', timeout=30)
                    payload = json.dumps({
                        'max_tokens': 512,
                        'model': 'gpt-3.5-turbo',
                        'messages': [{'role': 'user', 'content': prompt}]
                    })
                    headers = {
                        'Authorization': f'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx',
                        'User-Agent': 'Apifox/1.0.0 (https://apifox.com)',
                        'Content-Type': 'application/json'
                    }
                    conn.request('POST', '/v1/chat/completions', payload, headers)
                    res = conn.getresponse()
                    data = res.read().decode('utf-8')
                    data = json.loads(data)
                    response = data['choices'][0]['message']['content']
                    return response
                except Exception as e:
                    continue